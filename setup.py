"""
Setup and installation helper for Kalenjin Dictionary Processing Framework
"""

import subprocess
import sys
import os
from pathlib import Path

def check_python_version():
    """Check if Python version is compatible"""
    version = sys.version_info
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("‚ùå Python 3.8 or higher is required")
        print(f"   Current version: {version.major}.{version.minor}.{version.micro}")
        return False
    print(f"‚úÖ Python version: {version.major}.{version.minor}.{version.micro}")
    return True

def check_pip():
    """Check if pip is available"""
    try:
        subprocess.run([sys.executable, "-m", "pip", "--version"], 
                      check=True, capture_output=True)
        print("‚úÖ pip is available")
        return True
    except subprocess.CalledProcessError:
        print("‚ùå pip is not available")
        return False

def install_requirements():
    """Install requirements from requirements.txt"""
    requirements_file = Path("requirements.txt")
    
    if not requirements_file.exists():
        print("‚ùå requirements.txt not found")
        return False
    
    print("üì¶ Installing Python packages...")
    try:
        # Install basic requirements first
        basic_cmd = [
            sys.executable, "-m", "pip", "install", 
            "torch", "torchvision", "--index-url", "https://download.pytorch.org/whl/cu118"
        ]
        subprocess.run(basic_cmd, check=True)
        
        # Install remaining requirements
        cmd = [sys.executable, "-m", "pip", "install", "-r", str(requirements_file)]
        subprocess.run(cmd, check=True)
        print("‚úÖ All packages installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Installation failed: {e}")
        return False

def check_gpu_support():
    """Check if CUDA is available"""
    try:
        import torch
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "Unknown"
            print(f"‚úÖ CUDA available: {device_count} GPU(s)")
            print(f"   Primary GPU: {device_name}")
            return True
        else:
            print("‚ö†Ô∏è  CUDA not available - will use CPU (slower)")
            return False
    except ImportError:
        print("‚ö†Ô∏è  PyTorch not installed - cannot check GPU support")
        return False

def create_directories():
    """Create necessary directories"""
    dirs = ["output", "models", "temp"]
    
    for dirname in dirs:
        dir_path = Path(dirname)
        dir_path.mkdir(exist_ok=True)
        print(f"üìÅ Created directory: {dirname}")
    
    return True

def download_model(model_name="Qwen/Qwen2.5-VL-7B-Instruct"):
    """Download the VLM model"""
    print(f"ü§ñ Checking model availability: {model_name}")
    
    try:
        from huggingface_hub import snapshot_download
        from transformers import AutoTokenizer
        
        # Check if model exists
        print("üì• Downloading model (this may take a while)...")
        model_path = snapshot_download(repo_id=model_name, cache_dir="./models")
        print(f"‚úÖ Model downloaded to: {model_path}")
        
        return True
    except ImportError:
        print("‚ö†Ô∏è  huggingface_hub not available - model will download on first use")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Model download failed: {e}")
        print("üí° Model will be downloaded automatically on first use")
        return True

def test_installation():
    """Test if installation works"""
    print("\nüß™ Testing installation...")
    
    # Test PDF processing
    try:
        from scripts.pdf_to_images import PDFToImageConverter
        converter = PDFToImageConverter()
        print("‚úÖ PDF processing module works")
    except Exception as e:
        print(f"‚ùå PDF processing test failed: {e}")
        return False
    
    # Test VLM imports
    try:
        from llm.config import VLMConfig
        from llm.main import VLMProcessor
        config = VLMConfig()
        print("‚úÖ VLM modules work")
    except Exception as e:
        print(f"‚ùå VLM modules test failed: {e}")
        return False
    
    # Test parser
    try:
        from llm.parser.schemas import DictionaryEntry
        entry = DictionaryEntry(grapheme="test")
        print("‚úÖ Parser modules work")
    except Exception as e:
        print(f"‚ùå Parser modules test failed: {e}")
        return False
    
    print("‚úÖ All tests passed!")
    return True

def main():
    """Main setup function"""
    print("üöÄ Kalenjin Dictionary Processing Framework Setup\n")
    
    # Check prerequisites
    if not check_python_version():
        return 1
    
    if not check_pip():
        return 1
    
    # Create directories
    print("\nüìÅ Creating directories...")
    create_directories()
    
    # Install packages
    print("\nüì¶ Installing dependencies...")
    if not install_requirements():
        return 1
    
    # Check GPU support
    print("\nüñ•Ô∏è  Checking hardware support...")
    has_gpu = check_gpu_support()
    
    # Update environment file with GPU setting
    env_file = Path(".env")
    if env_file.exists():
        with open(env_file, "r") as f:
            content = f.read()
        
        if has_gpu:
            content = content.replace("VLM_DEVICE=cpu", "VLM_DEVICE=cuda")
            content = content.replace("CUDA_AVAILABLE=false", "CUDA_AVAILABLE=true")
        else:
            content = content.replace("VLM_DEVICE=cuda", "VLM_DEVICE=cpu")
        
        with open(env_file, "w") as f:
            f.write(content)
    
    # Download model (optional)
    if input("\nü§ñ Download VLM model now? (y/N): ").lower().startswith('y'):
        download_model()
    else:
        print("‚è≠Ô∏è  Skipping model download (will download on first use)")
    
    # Test installation
    if not test_installation():
        print("\n‚ùå Setup completed with errors")
        return 1
    
    print("\nüéâ Setup completed successfully!")
    print("\nüìã Next steps:")
    print("1. Place your Kalenjin dictionary PDF in the project root")
    print("2. Run demo: python demo.py")
    print("3. Or run full pipeline: python main.py pipeline your_dictionary.pdf")
    
    return 0

if __name__ == "__main__":
    exit(main())
